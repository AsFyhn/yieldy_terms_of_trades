{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "h1 {text-align: center;}\n",
    "p {text-align: center;}\n",
    "div {text-align: center;}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<h1>Data analysis</h1>\n",
    "\n",
    "<h3>Asbj√∏rn Fyhn & Emil Kolko Beckett</h3>\n",
    "<h3>Seminar: Topics in Sovereign Debt (Spring 2025)</h3>\n",
    "\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for next time: \n",
    "- section 5\n",
    "    - Remove observations after 2018\n",
    "    - Predict spreads: Beregn som i en binomial model: $S_{up}$ (value 1) og $S_{down}$ (recovery rate)\n",
    "    - Opstill figurer etc\n",
    "\n",
    "- Ryd op i analysis of process raw: \n",
    "    - flyt spread beregning til prossess raw \n",
    "    - Regn YTD rigtigt, lav en YTD_capped, dummy for capped\n",
    "    - Brug den nye RISK_REGION i stedet for coun_region_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatic reload of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import statsmodels.api as sm; \n",
    "import LinearModel as lm, estimation, tools\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import t, norm\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "# Function to determine the sign of a number\n",
    "sign = lambda x: math.copysign(1, x) # returns 1 if x > 0 else -1 if x < 0 else 0\n",
    "\n",
    "# Boolean options for saving figures and tables\n",
    "save_figures = True\n",
    "\n",
    "# create a list of hex colors for each region\n",
    "cmap = tools.colorway\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif' #'Century Gothic'\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "path_to_data = r'/Users/asbjornfyhn/Desktop/Seminar - sovergin debt/code/data/'\n",
    "path_to_output = r'/Users/asbjornfyhn/Desktop/Seminar - sovergin debt/code/output/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embi = pd.read_excel(path_to_data+'embi_regression_data.xlsx')\n",
    "# Create YTD properly and constant \n",
    "ysd_var = 'years_since_in_default' # years_since_new_default, years_since_in_default # last one yield better results\n",
    "df_embi['YEARS_SINCE_DEFAULT'] = df_embi[ysd_var].copy(deep=True) \n",
    "df_embi.loc[(df_embi[ysd_var]>=11) | (df_embi[ysd_var].isna()),'YEARS_SINCE_DEFAULT'] = 11.\n",
    "\n",
    "df_embi['constant'] = 1\n",
    "df_embi['spread'] = df_embi['spread'] *100\n",
    "# Renaming to make it compatible with the rest of the code\n",
    "df_embi = df_embi.rename(\n",
    "    columns={\n",
    "        'Gold_to_GDP':'gold_to_gdp', \n",
    "        'External Debt to GDP':'ex_debt_to_gdp', \n",
    "        'YEARS_SINCE_DEFAULT':'YTD',\n",
    "        'PCT_TOT_Real':'PCT_TOT_Real_5yr',\n",
    "        'VOL_TOT_Real':'VOL_TOT_Real_5yr',}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(path_to_data+'/prob_model_data.xlsx',index_col=[0,1])\n",
    "df['constant'] = 1\n",
    "\n",
    "bUnique = df.loc[(df.index.get_level_values(1)<='2000-01-01'),].index.get_level_values(0).unique()\n",
    "# df = df.loc[df.index.get_level_values(0).isin(bUnique),]\n",
    "df_embi['Date'] = pd.to_datetime(df_embi['Date'])\n",
    "df = df.loc[df.index.get_level_values(1) < df_embi['Date'].min()]\n",
    "\n",
    "print(f\"Number of defaults {df['New_default'].sum():.0f}\")\n",
    "df.loc[df['New_default']==1,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_str = 'New_default'\n",
    "x_str = ['constant','PCT_TOT_Real_5yr','VOL_TOT_Real_5yr','YTD','ex_debt_to_gdp','gold_to_gdp']\n",
    "x_labels_3y = ['constant','PCT_TOT_Real_3yr','VOL_TOT_Real_3yr','YTD','ex_debt_to_gdp','gold_to_gdp']\n",
    "\n",
    "pltDf = df.groupby(level=1)[['New_default','constant']].sum()\n",
    "pltDf1 = df.dropna(subset=[y_str]+x_str,how='any').groupby(level=1)[['New_default','constant']].sum()\n",
    "pltDf2 = df.dropna(subset=[y_str]+x_labels_3y,how='any').groupby(level=1)[['New_default','constant']].sum()\n",
    "\n",
    "# Calculate non-defaults\n",
    "non_defaults = pltDf['constant'] - pltDf['New_default']\n",
    "\n",
    "# Create the stacked bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Stack defaults on top\n",
    "ax.bar(pltDf.index.year, pltDf['New_default'], color='red', label='Defaults',alpha=0.4)\n",
    "ax.scatter(pltDf1.index.year, pltDf1['New_default'], color='black', label='Defaults',alpha=0.8, marker='s')\n",
    "ax.scatter(pltDf2.index.year, pltDf2['New_default'], color='white', label='Defaults',alpha=0.8, marker='x')\n",
    "# Plot non-defaults as the base\n",
    "# ax.bar(pltDf.index.year, non_defaults, bottom=pltDf['New_default'], color='blue', label='Non-Defaults', alpha=0.4)\n",
    "\n",
    "# Customize\n",
    "ax.set_ylabel('Number of Observations')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_title('Stacked Bar Chart of Defaults and Non-Defaults')\n",
    "ax.legend()\n",
    "\n",
    "# Rotate x-axis labels if needed\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logitus:\n",
    "    def __init__(self, df:pd.DataFrame, yLabel:str, xLabels:list, ):\n",
    "        self.yLabel = yLabel\n",
    "        self.xLabels = xLabels\n",
    "\n",
    "        self.df = deepcopy(df.dropna(subset=[yLabel]+xLabels,how='any'))\n",
    "        print(\n",
    "            self.df.index.get_level_values(1).max()\n",
    "        )\n",
    "        self.x = deepcopy(self.df[xLabels].values)\n",
    "        self.y = deepcopy(self.df[yLabel].values)\n",
    "\n",
    "    def describe_y(self,verbose:bool=True):\n",
    "        y = deepcopy(self.y)\n",
    "        self.no_of_obs = y.shape[0]\n",
    "        self.no_of_defaults = y[y==1].shape[0]\n",
    "        self.no_of_non_defaults = y[y==0].shape[0]\n",
    "        if verbose:\n",
    "            print(f\"Number of observations: {self.no_of_obs}\")\n",
    "            print(f\"Number of defaults: {self.no_of_defaults}\")\n",
    "            print(f\"Number of non-defaults: {self.no_of_non_defaults}\")\n",
    "\n",
    "    def remove_outliers(self, outliar_cut=3, indices=None):\n",
    "        X = deepcopy(self.x)\n",
    "        \n",
    "        if indices is None:\n",
    "            indices = [i for i in range(X.shape[1]) if self.xLabels[i] != 'constant']\n",
    "\n",
    "        # Calculate mean and standard deviation along axis 0 (across rows for each column)\n",
    "        means = np.mean(X[:, indices], axis=0)\n",
    "        stds = np.std(X[:, indices], axis=0)\n",
    "\n",
    "        # Create a boolean mask for rows where all values are within outliar_cut standard deviations\n",
    "        mask = np.all(np.abs(X[:,indices] - means) <= (outliar_cut * stds), axis=1)\n",
    "\n",
    "        # Apply the mask to filter the array\n",
    "        X_filtered = X[mask]\n",
    "        y_filtered = self.y[mask]\n",
    "\n",
    "        self.x = deepcopy(X_filtered)\n",
    "        self.y = deepcopy(y_filtered)\n",
    "\n",
    "    def run_logit(self, display:bool=True):\n",
    "        \"\"\" \"\"\"\n",
    "        # Get initial values for theta (using OLS)\n",
    "        theta0 = tools.starting_values(self.y, self.x)\n",
    "        \n",
    "        # \n",
    "        logit_res = estimation.estimate(\n",
    "            q = tools.q,\n",
    "            theta0=theta0,\n",
    "            y=self.y,\n",
    "            x=self.x,\n",
    "            cov_type='Sandwich',#'Sandwich',\n",
    "            options={'disp':display}\n",
    "        )\n",
    "\n",
    "        regRes = pd.DataFrame(index=['beta','se','t-stat'], columns=self.xLabels, data=[logit_res[col] for col in ['theta','se','t']]).T\n",
    "        degressFreedom = self.y.shape[0]-len(self.xLabels)-1\n",
    "        regRes['p'] = 2*(t.cdf(-abs(regRes['t-stat']), degressFreedom))\n",
    "        \n",
    "        self.logit_res = logit_res\n",
    "        self.regressio_results = regRes\n",
    "        return regRes\n",
    "\n",
    "    def ape(self):\n",
    "        \"\"\" \"\"\"\n",
    "        apeRes = pd.DataFrame(index=self.xLabels, columns=['ape'])\n",
    "        for i, label in enumerate(self.xLabels):\n",
    "            if label == 'constant':\n",
    "                continue\n",
    "            ape = tools.average_partial_effect(x=self.x, \n",
    "                                               betas=self.logit_res['theta'],\n",
    "                                               k=i)\n",
    "            apeRes.loc[label,'ape'] = ape\n",
    "            del ape\n",
    "        \n",
    "        return apeRes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['RISK_REGION']!='Middle East']\n",
    "outliar_cut = 2\n",
    "small_indices = [1,2]\n",
    "large_indices = [0,1,2,4,5]\n",
    "large_indices = [4]\n",
    "verbose = True\n",
    "dispaly = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_5yr_small = ['constant','PCT_TOT_Real_5yr','VOL_TOT_Real_5yr','YTD']\n",
    "l_5yr_small = logitus(df, yLabel='New_default', xLabels=x_5yr_small)\n",
    "l_5yr_small.remove_outliers(outliar_cut=outliar_cut, indices=small_indices)\n",
    "l_5yr_small.describe_y(verbose=verbose)\n",
    "_ = l_5yr_small.run_logit(display=False)\n",
    "ape_5yr_small = l_5yr_small.ape()\n",
    "print(ape_5yr_small)\n",
    "\n",
    "\n",
    "# # Run bootstrap\n",
    "# apes, ses = [],[]\n",
    "# for i, label in enumerate(x_5yr_small):\n",
    "#     if label == 'constant':\n",
    "#         continue\n",
    "#     res = tools.bootstrap_se_with_fit(l_5yr_small.x, l_5yr_small.y,k=i,n_boot=1000, seed=2)\n",
    "#     apes.append(res[0])\n",
    "#     ses.append(res[1])\n",
    "\n",
    "# print(apes)\n",
    "# print(ses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_3yr_small = ['constant','PCT_TOT_Real_3yr','VOL_TOT_Real_3yr','YTD']\n",
    "l_3yr_small = logitus(df, yLabel='New_default', xLabels=x_3yr_small)\n",
    "l_3yr_small.remove_outliers(outliar_cut=outliar_cut, indices=small_indices)\n",
    "l_3yr_small.describe_y(verbose=verbose)\n",
    "_ = l_3yr_small.run_logit(display=False)\n",
    "ape_3yr_small = l_3yr_small.ape()\n",
    "\n",
    "# # Run bootstrap\n",
    "# apes, ses = [],[]\n",
    "# for i, label in enumerate(x_3yr_small):\n",
    "#     if label == 'constant':\n",
    "#         continue\n",
    "#     res = tools.bootstrap_se_with_fit(l_3yr_small.x, l_3yr_small.y,k=i,n_boot=1000, seed=2)\n",
    "#     apes.append(res[0])\n",
    "#     ses.append(res[1])\n",
    "\n",
    "# print(apes)\n",
    "# print(ses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_5yr_full = ['constant','PCT_TOT_Real_5yr','VOL_TOT_Real_5yr','YTD','ex_debt_to_gdp','gold_to_gdp']\n",
    "l_5yr_full = logitus(df, yLabel='New_default', xLabels=x_5yr_full)\n",
    "l_5yr_full.remove_outliers(outliar_cut=outliar_cut, indices=large_indices)\n",
    "l_5yr_full.describe_y(verbose=verbose)\n",
    "_ = l_5yr_full.run_logit(display=False)\n",
    "ape_5yr = l_5yr_full.ape()\n",
    "\n",
    "# # Run bootstrap\n",
    "# apes, ses = [],[]\n",
    "# for i, label in enumerate(x_5yr_full):\n",
    "#     if label == 'constant':\n",
    "#         continue\n",
    "#     res = tools.bootstrap_se_with_fit(l_5yr_full.x, l_5yr_full.y,k=i,n_boot=1000, seed=2)\n",
    "#     apes.append(res[0])\n",
    "#     ses.append(res[1])\n",
    "\n",
    "# print(apes)\n",
    "# print(ses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_3yr_full = ['constant','PCT_TOT_Real_3yr','VOL_TOT_Real_3yr','YTD','ex_debt_to_gdp','gold_to_gdp']\n",
    "l_3yr_full = logitus(df, yLabel='New_default', xLabels=x_3yr_full)\n",
    "# l_3yr_full.remove_outliers(outliar_cut=2, indices=[1,2,4,5])\n",
    "l_3yr_full.remove_outliers(outliar_cut=outliar_cut, indices=large_indices)\n",
    "l_3yr_full.describe_y(verbose=verbose)\n",
    "_ = l_3yr_full.run_logit(display=False)\n",
    "ape_3yr = l_3yr_full.ape() \n",
    "\n",
    "\n",
    "# # Run bootstrap\n",
    "# apes, ses = [],[]\n",
    "# for i, label in enumerate(x_3yr_full):\n",
    "#     if label == 'constant':\n",
    "#         continue\n",
    "#     res = tools.bootstrap_se_with_fit(l_3yr_full.x, l_3yr_full.y,k=i,n_boot=1000, seed=2)\n",
    "#     apes.append(res[0])\n",
    "#     ses.append(res[1])\n",
    "\n",
    "# print(apes)\n",
    "# print(ses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRes = pd.concat(\n",
    "    [\n",
    "    l_3yr_full.regressio_results[['beta','se','p']].stack().to_frame().rename(columns={0:'3 Yr full'}).rename(index={'PCT_TOT_Real_3yr':'PCT_TOT','VOL_TOT_Real_3yr':'VOL_TOT'}),\n",
    "    l_3yr_small.regressio_results[['beta','se','p']].stack().to_frame().rename(columns={0:'3 Yr small'}).rename(index={'PCT_TOT_Real_3yr':'PCT_TOT','VOL_TOT_Real_3yr':'VOL_TOT'}),\n",
    "    l_5yr_full.regressio_results[['beta','se','p']].stack().to_frame().rename(columns={0:'5 Yr full'}).rename(index={'PCT_TOT_Real_5yr':'PCT_TOT','VOL_TOT_Real_5yr':'VOL_TOT'}),\n",
    "    l_5yr_small.regressio_results[['beta','se','p']].stack().to_frame().rename(columns={0:'5 Yr small'}).rename(index={'PCT_TOT_Real_5yr':'PCT_TOT','VOL_TOT_Real_5yr':'VOL_TOT'}),\n",
    "    ], axis=1)\n",
    "\n",
    "# add number of observations to the table\n",
    "dfRes.loc[('Total','Obs'), :] = [l_3yr_full.no_of_obs,\n",
    "                                             l_3yr_small.no_of_obs,\n",
    "                                             l_5yr_full.no_of_obs,\n",
    "                                             l_5yr_small.no_of_obs]\n",
    "# add number of defaults to the table\n",
    "dfRes.loc[('Defaults','Obs'), :] = [l_3yr_full.no_of_defaults,\n",
    "                                             l_3yr_small.no_of_defaults,\n",
    "                                             l_5yr_full.no_of_defaults,\n",
    "                                             l_5yr_small.no_of_defaults]\n",
    "# add number of non-defaults to the table\n",
    "dfRes.loc[('Non-Defaults','Obs'), :] = [l_3yr_full.no_of_non_defaults,\n",
    "                                             l_3yr_small.no_of_non_defaults,\n",
    "                                             l_5yr_full.no_of_non_defaults,\n",
    "                                             l_5yr_small.no_of_non_defaults]\n",
    "\n",
    "if save_figures:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        resLatexy = dfRes.to_latex()\n",
    "        warnings.simplefilter(\"default\")\n",
    "    for i in range(len(resLatexy)):\n",
    "        resLatexy = resLatexy.replace('NaN','-')\n",
    "    resLatexy = resLatexy.replace('Obs','')\n",
    "\n",
    "    with open(path_to_output+'logit_results.tex', 'w') as f:\n",
    "        f.write(resLatexy)\n",
    "    print('Saved results to logit_results.tex')\n",
    "\n",
    "dfRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ape = pd.concat([\n",
    "    ape_3yr.rename(index={'PCT_TOT_Real_3yr':'Chg. ToT','VOL_TOT_Real_3yr':'Vol. ToT', 'ex_debt_to_gdp':'Ext. Debt', 'gold_to_gdp':'Curr. res.'},columns={'ape':'3 Yr full'}),\n",
    "    ape_3yr_small.rename(index={'PCT_TOT_Real_3yr':'Chg. ToT','VOL_TOT_Real_3yr':'Vol. ToT', 'ex_debt_to_gdp':'Ext. Debt', 'gold_to_gdp':'Curr. res.'},columns={'ape':'3 Yr small'}),\n",
    "    ape_5yr.rename(index={'PCT_TOT_Real_5yr':'Chg. ToT','VOL_TOT_Real_5yr':'Vol. ToT', 'ex_debt_to_gdp':'Ext. Debt', 'gold_to_gdp':'Curr. res.'},columns={'ape':'5 Yr full'}), \n",
    "    ape_5yr_small.rename(index={'PCT_TOT_Real_5yr':'Chg. ToT','VOL_TOT_Real_5yr':'Vol. ToT', 'ex_debt_to_gdp':'Ext. Debt', 'gold_to_gdp':'Curr. res.'},columns={'ape':'5 Yr small'}), \n",
    "    ], axis=1).drop(index=['constant'], axis=0)\n",
    "\n",
    "if save_figures:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", )\n",
    "        apeLatex = ape.to_latex()\n",
    "        warnings.filterwarnings(\"default\")\n",
    "    with open(path_to_output+'ape_latex.tex', 'w') as f:\n",
    "        f.write(apeLatex)\n",
    "\n",
    "print(ape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict spreads for EMBI countries\n",
    "\n",
    "Method is based on Hull exercise 24.6 in which he calculates the historical one-year hazard rate from cumulative default rates. From there he uses an assumption of recovery rate of 40% to calculate the spread. \n",
    "\n",
    "    The spread to compensate for default are 60% of these (red: one-year hazard rate).\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_3yr_full.xLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embi[['Date']+l_3yr_full.xLabels].dropna()['Date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embi.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logit = df_embi.groupby(['Date','COUNTRY_CODE']).agg({'PCT_TOT_Real_3yr':'first', 'VOL_TOT_Real_3yr':'first','spread':'median',\n",
    "                                                         'treasury_yield':'first','slope':'first','VIX':'first',\n",
    "                                                         'rating_rank':'first','RISK_REGION':'first','rating_sp':'first','ex_debt_to_gdp':'first','gold_to_gdp':'first','YTD':'first','constant':'first'}).reset_index()\n",
    "\n",
    "df_plot = deepcopy(df_logit)\n",
    "\n",
    "# l_3yr_full.regressio_results\n",
    "Xembi = df_plot[l_3yr_full.xLabels]\n",
    "\n",
    "# Predict probabilities of default\n",
    "yhat = tools.predict(l_3yr_full.logit_res['theta'], Xembi.values)\n",
    "\n",
    "df_plot['pred_prob_default'] = yhat\n",
    "\n",
    "# Calculate the predicted spread\n",
    "p = df_plot['pred_prob_default'].copy()\n",
    "rr = 0.4 # recovery rate\n",
    "lgd = 1 - rr  # loss given default\n",
    "df_plot['pred_spreads'] = p*lgd *10_000\n",
    "\n",
    "# df_embi.columns.values\n",
    "df_plot[['pred_prob_default','pred_spreads','spread']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot.dropna(subset='pred_spreads')['Date'].value_counts()\n",
    "df_plot.dropna(subset='pred_spreads')['COUNTRY_CODE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_plot['spread_lead'] = df_plot.groupby('COUNTRY_CODE')['spread'].transform(lambda x: x.shift(1))\n",
    "# df_plot['Date_lead'] = df_plot['Date'] + pd.DateOffset(years=1)\n",
    "\n",
    "dfPlot = df_plot.loc[(df_plot[['pred_spreads','spread','rating_rank']].notna().all(axis=1)), :] # OBS: Should we use pred_spreads_lead\n",
    "dfPlot.loc[dfPlot['COUNTRY_CODE']=='ARM',['Date','COUNTRY_CODE','spread','pred_spreads']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPlot = df_plot.loc[(df_plot[['pred_spreads','spread']].notna().all(axis=1)), :] # OBS: Should we use pred_spreads_lead\n",
    "\n",
    "# Remove if pred_spreads exceeds 3 std deviations\n",
    "dfPlot = dfPlot.loc[\n",
    "    (dfPlot['pred_spreads'] < dfPlot['pred_spreads'].mean() + 3*dfPlot['pred_spreads'].std()) & \n",
    "    (dfPlot['pred_spreads'] > dfPlot['pred_spreads'].mean() - 3*dfPlot['pred_spreads'].std()), :]\n",
    "\n",
    "# store the EMBI spread and predicted spreads in separate variables\n",
    "spread = dfPlot['spread'].values \n",
    "pred_spread = dfPlot['pred_spreads'].values\n",
    "region = dfPlot['RISK_REGION'].values\n",
    "year = dfPlot['Date'].dt.year.values\n",
    "rating_rank = dfPlot['rating_rank'].values\n",
    "rating = dfPlot['rating_sp'].values\n",
    "controls = dfPlot[['RISK_REGION','pred_prob_default']].values\n",
    "\n",
    "# remove where spread exceeds 25000\n",
    "threshold = 1_000 #2500\n",
    "mask = (spread > 100) & (spread < threshold) & (region != 'Middle East') & (rating != 'D')\n",
    "region = region[mask]\n",
    "pred_spread = pred_spread[mask]\n",
    "spread = spread[mask]\n",
    "year = year[mask]\n",
    "rating_rank = rating_rank[mask]\n",
    "rating = rating[mask]\n",
    "controls = controls[mask, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPlot = dfPlot.loc[(dfPlot['spread']<1_000)&(dfPlot['spread']>1_00) & (dfPlot['RISK_REGION']!='Middle East') & (dfPlot['rating_sp']!='D'),:]\n",
    "# dfPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS fit \n",
    "x = dfPlot[['pred_spreads','VIX','treasury_yield','slope']].values\n",
    "add_constant = False\n",
    "olsReg = sm.OLS(endog=dfPlot['spread'], exog=x).fit(cov_type='HC3') \n",
    "olsReg.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "\n",
    "# Scatter plot of predicted spreads vs. EMBI spread\n",
    "ax.scatter(pred_spread, spread, color=cmap[0], alpha=0.5, label='Data', s=20)\n",
    "\n",
    "# OLS fit \n",
    "x = np.linspace(pred_spread.min(), pred_spread.max(), 100)\n",
    "add_constant = False\n",
    "olsReg = sm.OLS(endog=spread, exog=pred_spread).fit() if not add_constant else sm.OLS(endog=spread, exog=sm.add_constant(pred_spread)).fit()\n",
    "y_hat = olsReg.predict(x) if not add_constant else olsReg.predict(sm.add_constant(x))\n",
    "res = olsReg.resid\n",
    "ax.plot(x, y_hat, color='black', linestyle='--', label='OLS Fit', alpha=0.8)\n",
    "# Write the coefficient and r-squared value on the plot with italic\n",
    "ax.text(0.75, 0.25, f'Coefficient: {olsReg.params[0]:.2f}', transform=ax.transAxes, fontsize=10,fontdict={'style':'italic'})\n",
    "ax.text(0.75, 0.20, f'R-squared: {olsReg.rsquared:.2f}', transform=ax.transAxes, fontsize=10,fontdict={'style':'italic'})\n",
    "\n",
    "# Set labels\n",
    "ax.set_xlabel('Predicted Spreads (bps)')\n",
    "ax.set_ylabel('EMBI Spread (bps)')\n",
    "\n",
    "# Remove the top and right spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(title='Region', loc='upper right', bbox_to_anchor=(1, 1), fontsize='small', frameon=False,) \n",
    "\n",
    "# Set x and y limits\n",
    "ax.set_ylim(0,threshold*1.05)\n",
    "ax.set_xlim(pred_spread.min()*(1 - 0.2*sign(pred_spread.min())),pred_spread.max()*(1 + 0.2*sign(pred_spread.max())))\n",
    "# Gridlines\n",
    "ax.grid(which='major', linestyle='--', linewidth=0.5)\n",
    "\n",
    "if save_figures:\n",
    "    plt.savefig(path_to_output+'scatter_predicted_spreads.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(12, 4))\n",
    "\n",
    "for i, reg in enumerate(np.unique(region)):\n",
    "    ax[0].bar(i, np.mean(pred_spread[region==reg]-spread[region==reg]), alpha=1, label=reg, color=cmap[i])\n",
    "    # ax[0].errorbar(i, np.mean(pred_spread[region==reg]-spread[region==reg]), \n",
    "    #                yerr=np.std(pred_spread[region==reg]-spread[region==reg]), fmt='o', color='black')\n",
    "\n",
    "for i, y in enumerate(np.unique(year)):\n",
    "    ax[1].bar(i, np.mean(pred_spread[year==y]-spread[year==y]), alpha=1, label=y, color=cmap[i])\n",
    "    # ax[1].errorbar(i, np.mean(pred_spread[year==y]-spread[year==y]), \n",
    "    #                yerr=np.std(pred_spread[year==y]-spread[year==y]), fmt='o', color='black')\n",
    "\n",
    "for i, rat in enumerate(np.unique(rating_rank)):\n",
    "    ax[2].bar(i, np.mean(pred_spread[rating_rank==rat]-spread[rating_rank==rat]), alpha=1, label=y, color=cmap[i])\n",
    "    # ax[2].errorbar(i, np.mean(pred_spread[rating_rank==rat]-spread[rating_rank==rat]), \n",
    "    #                yerr=np.std(pred_spread[rating_rank==rat]-spread[rating_rank==rat]), fmt='o', color='black')\n",
    "\n",
    "# Set xticks \n",
    "ax[0].set_xticks(range(len(np.unique(region))))\n",
    "ax[0].set_xticklabels([reg if ' ' not in reg else reg.replace(' ','\\n') for reg in np.unique(region)], rotation=45)\n",
    "ax[1].set_xticks(range(len(np.unique(year))))\n",
    "ax[1].set_xticklabels(np.unique(year+1), rotation=45)\n",
    "ax[2].set_xticks(range(len(np.unique(rating))))\n",
    "ax[2].set_xticklabels(np.unique(rating)[::-1], rotation=45)\n",
    "\n",
    "\n",
    "# Set labels\n",
    "ax[0].set_ylabel('Mean Predicted - Actual Spread')\n",
    "\n",
    "# Remove spines \n",
    "ax[0].spines['top'].set_visible(False)\n",
    "ax[0].spines['right'].set_visible(False)\n",
    "ax[1].spines['top'].set_visible(False)\n",
    "ax[1].spines['right'].set_visible(False)\n",
    "ax[2].spines['top'].set_visible(False)\n",
    "ax[2].spines['right'].set_visible(False)\n",
    "\n",
    "# Add horizontal gridlines\n",
    "ax[0].grid(axis='y', linestyle='--', linewidth=0.5)\n",
    "ax[1].grid(axis='y', linestyle='--', linewidth=0.5)\n",
    "ax[2].grid(axis='y', linestyle='--', linewidth=0.5)\n",
    "\n",
    "if save_figures:\n",
    "    plt.savefig(path_to_output+'bar_predicted_spreads.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Below code creates problems as it re-defines spreads...***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = np.quantile(controls[:,1], [0, 0.25, 0.5, 0.75, 1], axis=0,)\n",
    "\n",
    "# Create a bar chart \n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "\n",
    "for i in range(4):\n",
    "    quantile = i*0.25\n",
    "    mask = (controls[:,1] >= quantiles[i]) & (controls[:,1] < quantiles[i+1])\n",
    "    spread_quantile = spread[mask]\n",
    "    pred_spread_quantile = pred_spread[mask]\n",
    "    # Predicted spread for the quantile\n",
    "    ax.bar(quantile-0.02, np.mean(spread_quantile), width=0.1, label=f'Acutal Spreads', color=cmap[0])\n",
    "    # ax.errorbar(quantile-0.02, np.mean(spread_quantile), yerr=np.std(spread_quantile), fmt='o', color='black', alpha=0.5)\n",
    "    # Actual spread for the quantile\n",
    "    ax.bar(quantile+0.02, np.mean(pred_spread_quantile), width=0.1, label=f'Predicted Spreads', alpha=0.5,color=cmap[1])\n",
    "    # ax.errorbar(quantile+0.02, np.mean(pred_spread_quantile), yerr=np.std(pred_spread_quantile), fmt='o', color='black', alpha=0.5)\n",
    "    \n",
    "# Remvoe spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Set labels\n",
    "ax.set_xlabel('Quantiles of Predicted Probability of Default')\n",
    "ax.set_ylabel('Spread (bps)')\n",
    "\n",
    "# Only unique labels in the legend\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "ax.legend(by_label.values(), by_label.keys(), loc='upper left', bbox_to_anchor=(0.75, 1), fontsize='small', frameon=False)\n",
    "# Gridlines\n",
    "ax.grid(which='major', linestyle='--', linewidth=0.5)\n",
    "# Set x-ticks\n",
    "ax.set_xticks(np.arange(0, 4/4, 1/4))\n",
    "# Set x-tick labels\n",
    "ax.set_xticklabels([f'{i+1}' for i in range(4)])\n",
    "\n",
    "# Set y-limits\n",
    "ax.set_ylim(0, max(np.mean(spread_quantile), np.mean(pred_spread_quantile))*1.2)\n",
    "\n",
    "if save_figures:\n",
    "    plt.savefig(path_to_output+'bar_predicted_spreads_quantiles.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
